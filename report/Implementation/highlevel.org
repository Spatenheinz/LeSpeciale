*** De Bruijn Indicies
From the description of the type semantics in \ref{sec:typing}, we notice that for a type to be correctly checked it must have definitional equality.
Using De Bruijn indices makes this process a lot easier since it allows for considering \alpha-equivalence as syntactical equivalence. Specifically with De Bruijn indices, when $\beta\eta$ normal form has been achieved, we get \alpha-equivalence for free.
De Bruijn indices further makes the process of beta-reduction easier, as variables dont have to be renamed, i.e. \alpha-convertion.
Consider application
\( (\lambda x. \lambda y. x y) y\)
If we were to do direct substitution in
\( \subst{y}{x}{(\lambda y. x y)} \)
we would get
\( \lambda y. y y \).
This changes the meaning of the term.
De Brujin indices instead swap each bound variable with a positive integer.
The meaning of the integer $n$ is then constituted by the $n^{th}$ enclosing abstraction, \Pi, \lambda or $let$.
Consider
\( \lambda x . \lambda y . \lambda y . x y \) and \( \lambda y . \lambda x . \lambda x . y x \)
which are on \(\beta\eta\)-long form and \alpha-equivalent but not syntactically identical.
Using De Bruijn notation we get:
\( \lambda \lambda \lambda 2 0\) for both, since the function point in the inner application is described by the second innermost binder, starting from zero,  whilst the argument for the application is 0, since it is captured by the innermost abstraction.
If we again consider \( (\lambda x. \lambda y. x y) y\) the De bruijn representation is
\( (\lambda \lambda 1 0) y\) and by beta reduction becomes \(\lambda y 0\).
We only consider De Bruijn notation for bound variables, thus we get around any capture avoiding complications.
We could potentially also consider using De Bruijn indices for free variables, however this would complicate the code as this would require lifting binders.
Although it could be interesting to consider De Bruijn levels since these are not relative to the scope.

We also consider De Bruijn indices for other binders such as program definitions:
#+begin_src
(function sc_arith_add_nary ((t1 term) (t2 term)) term
  (a.+ t1 t2))
#+end_src
This program definition will get converted into:
#+begin_src
(function sc_arith_add_nary ((term) (term)) term
  (a.+ _1 _0))
#+end_src
Where _ prefixes De Bruijn index to distinguish them from integers.

Similarly in pattern matching, when a constructor is applied to multiple arguments, the following
#+begin_src
  (match t
    ((apply t1 t2)
      (let t12 (getarg f t1)
        (ifequal t12 l tt (nary_ctn f t2 l))))
    (default ff))
#+end_src
gets converted into:
#+begin_src
  (match t
    ((apply 2)
      (let (getarg f _1)
        (ifequal _0 l tt (nary_ctn f _1 l))))
    (default ff))
#+end_src
Notice here that the arguments ~t1~ and ~t2~ are substituted by ~2~ as argument for the constructor as we need to save the number of arguments to not lose the meaning of the constructor, as it must be fully applied. In the example we also converted the binder of the "let".

*** Explicit substitutions
We have already touched upon substitution, but another matter at which we shall consider is the sheer cost of direct substitution.
Performing direct substitution on terms we cause an explosion in the size of the term and unnecessarily waste both memory and execution time because we have to copy the struct at each occurence and also traverse terms multiple times.
On the other hand explicit substitution allow us to not generate anything unnecessarily large and keep the computation at a minimum.

We consider a substitution which is lazy, meaning we use the result of a substititution, by lookup, when necessary and then proceed, but does not generate any explicit substitutions.
Specifically we use Rust closures to capture \Gamma.


*** Normalization by Evaluation
As mentioned, we consider checking of types w.r.t. definitional equality. To do this we must have terms on normal forms, and we use the Normalization by Evaluation (NbE) for this.
NbE is a process first introduced by Berger and Schwichtenberg\cite{nbe} for efficient normalization of simply typed calculus, but it has since been refined for other systems in Barendregt's lambda cube.
This implementation is inspired by Christensens writeup "Checking Dependent Types with Normalization by Evaluation"\cite{nbehs}.
The technique ties a connection between syntax and semantics.

The process of evaluating a programming language amounts to either compilation to machine code followed by execution, or by the use of an interpreter.
In both cases evaluation gives meaning to said program.
For instance, if the result of an interpretation is a number then the number constitutes the meaning of that program.
The meaning may not be concrete but can also be functions, and since we consider typechecking that can invoke evaluation values and types live in a similar domain.
Evaluation of LFSC can result in values for the built in types *type* and *kind* as well as function types such as \Pi types.
Evaluation in general is only sensible for closed terms, but we must also consider how to handle open terms.

The process of normalization on the other hand is to transform a program into its normal form.
Letting \(nf(M)\) denote the normal form of term $M$ with \(\context M : A\),
then the following properties must hold:
- \(\context nf(M) : A\)
- \(\context nf(nf(M)) = nf(M) \)
- \( \llbracket nf(M) \rrbracket = \llbracket M \rrbracket\)
That is the normal form has the same type as the original term.
The normal form is idempotent and cannot be further normalized.
Finally the meaning does not change when normalizing a term.
Many functions have these properties, so we further consider the normal form to be expressions which contains no redexes.
A redex is a function type applied directly to an argument.
Specifically the normalization is considered with respect to \beta-reduction.
As already mentioned the process of \beta-reduction is slow, since it requires multiple traversals of terms.
Instead we interpret the understanding of finding a normal form can as evaluation on open terms.
The result of such an evaluation will not have any meaning; hence not be a value but rather a modified term with possibly unknown values.
We denote these as neutral expressions. A neutral expression in general may be free variables,
or application where the function point is a neutral, or, in the case of LFSC, a hole.
By including neutrals, we can in fact perform evaluation on open terms.
We define an evaluation reflection function $T \longrightarrow \llbracket T \rrbracket$ giving meaning to terms.
Then to convert the meaning back into a normal form, we define a reification function $\llbracket T \rrbracket \longrightarrow T$.
A normal form is then obtained by evaluation followed by reification. We describe the concrete implementation of these in \ref{sec:eval} and \ref{sec:readback}.
